# PageIndex PoC: Financial Filing Q&A System

A re-implementation of [PageIndex](https://github.com/getmetal/pageindex) for querying and analyzing financial filings across multiple companies and years using semantic tree structures and hybrid retrieval.

## Overview

This project implements a vectorless, reasoning-based RAG system specifically designed for multi-year, multi-company financial documents (e.g., Form 20-F filings). Instead of traditional vector similarity search, it leverages **PageIndex's semantic tree structures** combined with **LLM-powered tree navigation** for accurate, cited answers.

### Key Features

- ğŸ“„ **Multi-document support**: Ingest and query across multiple Form 20-F filings from different companies and years
- ğŸŒ³ **Semantic tree indexing**: Generates hierarchical document structures with semantic summaries
- ğŸ” **Hybrid retrieval**: Combines value-based node scoring with LLM tree search for robust results
- ğŸ’¾ **Embedding storage**: Optional embeddings (768-dim) for value-based search via Ollama
- ğŸ“ **Cited answers**: LLM generates answers with page/node citations for verification
- âš¡ **CLI ingest pipeline**: Single-document or batch ingestion with metadata auto-detection
- ğŸ§ª **Comprehensive tests**: 56 unit tests covering all core modules
- ğŸ³ **Docker-ready**: Ollama embeddings service included in docker-compose

## Architecture

### Core Components

```
backend/
â”œâ”€â”€ config.py          # Centralized configuration
â”œâ”€â”€ database.py        # SQLite schema and connection
â”œâ”€â”€ models.py          # Pydantic schemas
â”œâ”€â”€ llm/
â”‚   â””â”€â”€ client.py      # OpenRouter LLM wrapper
â”œâ”€â”€ ingest/
â”‚   â”œâ”€â”€ metadata.py    # PDF filename parser (TICKER_DOCTYPE_YEAR)
â”‚   â”œâ”€â”€ chunker.py     # Token-aware text chunking
â”‚   â”œâ”€â”€ embedder.py    # Ollama embedding client
â”‚   â””â”€â”€ pipeline.py    # Full ingest orchestration
â””â”€â”€ corpus/
    â””â”€â”€ manager.py     # Document CRUD operations
```

### Data Flow

```
PDF â†’ PageIndex Tree Generation â†’ Chunking & Embedding â†’ SQLite Storage
                                           â†“
Query â†’ Value Search + LLM Search â†’ Hybrid Merge â†’ Answer + Citations
```

## Installation

### Prerequisites

- Python 3.9+
- Docker & Docker Compose (for Ollama embeddings)
- OpenRouter API key

### Setup

1. **Clone and setup virtual environment:**
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   ```

2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   pip install pytest pytest-asyncio  # For testing
   ```

3. **Start Ollama service:**
   ```bash
   docker-compose up -d
   ```
   This starts Ollama on port 11435 with `nomic-embed-text-v2-moe` model pre-loaded.

4. **Configure environment:**
   ```bash
   cp .env.example .env
   ```
   Edit `.env` with your OpenRouter API key:
   ```
   OPENAI_API_KEY=sk-or-v1-xxxxx
   OPENAI_BASE_URL=https://openrouter.ai/api/v1
   LLM_MODEL=openai/gpt-5.2
   ```

## Usage

### Ingestion Pipeline

Ingest financial PDFs with automatic metadata detection from filenames.

**Single document:**
```bash
python -m scripts.ingest --pdf data/INFY_20F_2022.pdf --company "Infosys"
```

**Batch ingestion:**
```bash
python -m scripts.ingest --dir data/pdfs/ --company-map mapping.json
```

Where `mapping.json`:
```json
{
  "INFY_20F_2022.pdf": "Infosys",
  "INFY_20F_2023.pdf": "Infosys",
  "TCS_20F_2022.pdf": "Tata Consultancy Services"
}
```

**Force re-ingestion** (skip duplicate check):
```bash
python -m scripts.ingest --pdf data/INFY_20F_2022.pdf --company "Infosys" --force
```

### Filename Format

PDFs must follow the naming convention: `TICKER_DOCTYPE_YEAR.pdf`

Examples:
- `INFY_20F_2022.pdf` â†’ Infosys, Form 20-F, 2022
- `TCS_10K_2023.pdf` â†’ TCS, Form 10-K, 2023

### Query Interface

(Implementation pending - Phase 5 in roadmap)

```bash
python -m scripts.query "What was the revenue in 2022?"
```

## Testing

Run the comprehensive test suite:

```bash
# All tests
python -m pytest tests/ -v

# Specific module
python -m pytest tests/test_pipeline.py -v

# With coverage
python -m pytest tests/ --cov=backend
```

### Test Coverage

- **test_metadata.py** (12 tests): Filename parsing, normalization, edge cases
- **test_chunker.py** (8 tests): Token counting, splitting, overlap handling
- **test_database.py** (5 tests): Schema, CRUD, constraints
- **test_corpus.py** (12 tests): Document operations, tree retrieval
- **test_embedder.py** (3 tests): Embedding generation, batching
- **test_pipeline.py** (16 tests): End-to-end ingest flow with mocked externals

## Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `OPENAI_API_KEY` | (required) | OpenRouter API key |
| `OPENAI_BASE_URL` | https://openrouter.ai/api/v1 | LLM endpoint |
| `LLM_MODEL` | openai/gpt-5.2 | Model to use |
| `OLLAMA_URL` | http://localhost:11435 | Ollama service endpoint |
| `OLLAMA_MODEL` | nomic-embed-text-v2-moe | Embedding model |
| `DB_PATH` | data/pageindex.db | SQLite database location |
| `CHUNK_MAX_TOKENS` | 512 | Max tokens per chunk |
| `CHUNK_OVERLAP_TOKENS` | 64 | Token overlap between chunks |
| `CHUNK_MIN_TOKENS` | 32 | Minimum chunk size |

### Database Schema

Three-table design optimized for retrieval:

- **documents**: Metadata (ticker, fiscal_year, doc_type, company_name, page_count)
- **trees**: Full PageIndex tree JSON + derived structures (tree_no_text, node_map)
- **chunks**: Searchable text chunks with 768-dim embeddings (BLOB)

Unique constraint on `(ticker, fiscal_year, doc_type)` prevents duplicates.

## Project Structure

```
pageindex_poc/
â”œâ”€â”€ backend/                 # Core backend modules
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ ingest.py           # Ingestion CLI
â”œâ”€â”€ tests/                  # Unit tests (56 tests)
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ context/            # Reference materials
â”‚   â””â”€â”€ design/             # Design documents (8 docs)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ uploads/            # Ingested PDFs
â”‚   â””â”€â”€ pageindex.db        # SQLite database
â”œâ”€â”€ docker-compose.yaml     # Ollama service
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ README.md               # This file
```

## Development Roadmap

### âœ… Completed
- [x] Design & architecture documentation
- [x] Ingestion pipeline (PDF â†’ tree â†’ chunks â†’ embeddings â†’ SQLite)
- [x] Metadata parser (TICKER_DOCTYPE_YEAR format)
- [x] Token-aware chunker with overlap
- [x] Ollama embeddings integration
- [x] SQLite database with cascading deletes
- [x] CLI ingestion script (single & batch)
- [x] 56 comprehensive unit tests (100% passing)

### ğŸš§ In Progress / Planned
- [ ] Retrieval pipeline (value search + LLM tree search)
- [ ] Hybrid merge & deduplication
- [ ] Answer generation with citations
- [ ] FastAPI backend endpoints (`/corpus`, `/ingest`, `/query`)
- [ ] Frontend integration (Streamlit)
- [ ] End-to-end testing with real Form 20-F filings

## Troubleshooting

### Ollama Connection Issues
```bash
# Check if Ollama is running
curl http://localhost:11435/api/tags

# Restart Ollama service
docker-compose restart ollama-pageindex
```

### Database Corruption
```bash
# Reset database (WARNING: Deletes all data)
rm data/pageindex.db
python -m scripts.ingest --pdf data/sample.pdf --company "Test"
```

### OpenRouter API Errors
- Verify API key in `.env` (format: `sk-or-v1-...`)
- Check account has sufficient credits
- Ensure base URL is `https://openrouter.ai/api/v1`

## Design References

See `docs/design/` for detailed documentation:
- `01_project_overview.md` - Scope and key decisions
- `02_architecture.md` - Component diagrams and flow
- `03_data_model.md` - Database schema and node scoring formulas
- `04_ingest_pipeline.md` - Ingestion workflow details
- `05_retrieval_pipeline.md` - Query and retrieval strategy
- `06_api_contract.md` - REST API specification
- `07_frontend_adaptation.md` - Frontend integration notes
- `08_implementation_plan.md` - Phased development plan

## License

[Specify your license here]

## Contributing

[Contribution guidelines]

## Contact

For questions or issues, please open an issue or contact the development team.
